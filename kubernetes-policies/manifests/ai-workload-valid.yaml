# Valid AI Workload - Passes all OPA policies
# This manifest demonstrates compliant ML deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-model-v2
  namespace: ml-inference
  labels:
    app: recommendation-engine
    app-type: ml-model
    team: ml-platform-team
    model-name: product-recommender
    model-version: "2.1.0"
    model-framework: tensorflow
    data-classification: internal
    cost-center: ml-budget
    network-policy-applied: "true"
  annotations:
    # Model governance
    model-registry-url: "mlflow.company.internal/models/recommender/v2.1"
    model-training-date: "2024-12-01"
    approved-by: "ml-governance-team"
    
    # Monitoring
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
    
    # Data protection
    gdpr-compliant: "true"
    data-retention-days: "90"
    purpose-of-processing: "personalized-recommendations"
    lawful-basis: "legitimate-interest"
    training-data-source: "s3://ml-data/processed/products"
    storage-encryption-enabled: "true"
    
    # GPU resources
    gpu-justification: "TensorFlow model serving requires GPU for low-latency inference"
    
    # Production approval
    production-approved-by: "platform-lead"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recommendation-engine
  template:
    metadata:
      labels:
        app: recommendation-engine
        version: "2.1.0"
    spec:
      priorityClassName: ml-inference-priority
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: model-server
        image: ml-registry.company.com/tensorflow-serving:2.14-gpu
        ports:
        - containerPort: 8501
          name: rest-api
        - containerPort: 8500
          name: grpc-api
        - containerPort: 8080
          name: metrics
        env:
        - name: MODEL_NAME
          value: "product-recommender"
        - name: MODEL_BASE_PATH
          value: "/models"
        - name: TF_CPP_MIN_LOG_LEVEL
          value: "2"
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: "1"  # Matches request
        volumeMounts:
        - name: model-storage
          mountPath: /models
          readOnly: true
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        livenessProbe:
          httpGet:
            path: /v1/models/product-recommender
            port: rest-api
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /v1/models/product-recommender/versions/2
            port: rest-api
          initialDelaySeconds: 20
          periodSeconds: 5
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: ml-models-encrypted-pvc
      nodeSelector:
        gpu-type: t4  # Using T4 for inference (not A100/V100)
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
# NetworkPolicy for the deployment
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: recommendation-model-netpol
  namespace: ml-inference
spec:
  podSelector:
    matchLabels:
      app: recommendation-engine
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: api-gateway
    ports:
    - protocol: TCP
      port: 8501
    - protocol: TCP
      port: 8500
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: ml-training  # Can access training namespace
    ports:
    - protocol: TCP
      port: 443
  - to:
    - podSelector:
        matchLabels:
          app: model-registry
    ports:
    - protocol: TCP
      port: 5000